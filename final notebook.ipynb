{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Discourse\n",
    "\n",
    "### **Finding Numerical Ratings of Customer Sentiments using GTE Sentence Trasformer**\n",
    "\n",
    "by Vinicius Ambrosi, Gilyoung Cheong, Dohoon Kim, Hannah Lloyd\n",
    "\n",
    "**Abstract**. There is a wealth of discourse on companies and their products on social media platforms and online forums. While many approaches leverage analytical techniques to gauge audience sentiment through online discourse, they lack the ability to be both targeted and customizable while maintaining complex analytical integrity. We used Sentence Trasformer, the state-of-art text embedding API, with GTE (General Text Embeddings with Multi-stage Contrastive Learning) pretrained model, to vectorize 792,021 comments from Google Reviews into a 1024-dimensional vector space. We then used these vectors as features to train our models and develop rating models that rate a given comment from 1 to 5 stars. Each vector provides 1024 features for a review comment, with the target variable being the rating. We developed our models using Logistic Regression (One vs. Rest), k-Nearest Neigbor, XGBoost, and a Feedforward Neural Network with 30 hidden layers using ReLU activation after appropriate train-test splits. To address the biased nature of the training data, which favors 4 or 5-star ratings, we significantly improved our models by random under-sampling. To corroborate the performance of our models, we test them on another review dataset unrelated to Google Reviews for the same business. Our experiemental results provide how to use pre-trained model to extract numerical values of consumer sentiments from online comments without extra cost of pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S1.$ Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online comments and reviews have grown increasingly vital in shaping consumer decisions, particularly in the aftermath of the COVID-19 pandemic. Numerous studies, including those by [[1]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.865702/full), [[3]](https://link.springer.com/chapter/10.1007/978-981-19-5443-6_1), [[8]](https://ieeexplore.ieee.org/document/8970492), [[9]](https://www.sciencedirect.com/science/article/pii/S0747563210000907), [[10]](https://ieeexplore.ieee.org/document/8631160), and [[11]](https://www.sciencedirect.com/science/article/abs/pii/S1567422320300570) have underscored the significance of analyzing consumer sentiments within the realms of e-commerce and tourism. The importance of these sentiments has been highlighted, showing that understanding consumer feedback can provide valuable insights into market trends and customer preferences. In light of these findings, this report utilizes Natural Language Processing (NLP) and Machine Learning (ML) techniques to construct predictive models capable of assessing and rating comments provided by consumers. By employing these advanced analytical methods, we aim to enhance the accuracy and effectiveness of sentiment analysis in understanding and forecasting consumer behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1. NLP Methodology: GTE Sentence Transformer**. We use Sentence Transformer, the state-of-the-art text embedding NLP, to vectorize 792,021 pre-processed comments from Google Reviews about a specific business (Costco). These vectors then serve as input features for building predictive models for ratings. The pretrained model for our sentence transformer is GTE (General Text Embeddings with Multi-stage Contrastive Learning) developed by Alibaba Group NLP team in [[5]](https://arxiv.org/abs/2308.03281). The earliest Sentence Transfomer (Sentence-BERT) was was developed as outlined in [[6]](https://arxiv.org/abs/1908.10084) by Reimers and Gurebych and is based on BERT, which stands for Bidirectional Encoder Representations from Transformers, introduced in [[2]](https://arxiv.org/abs/1810.04805) by Google.\n",
    "\n",
    "BERT was revolutionary because it provided vector representations of sentences, with each subword token encoded into a vector that retains its relationship with the entire input context. This contextual awareness enabled BERT to excel in tasks such as question answering, where it generates answers based on user-provided context. \n",
    "\n",
    "However, using BERT for sentence clustering posed challenges because it required providing the full context every time a sentence was vectorized. This limitation made BERT less suitable for building predictive models that need to be tested on unknown data. Additionally, since BERT encodes each subword of a sentence into a vector, a sentence corresponds to a sequence of vectors, not a single vector, which requires significant storage. Various attempts to address these issues involved encoding each sentence without context and using either the encoded vector of the first special token (called the [CLS] token) or the average vector of the sequence of encoded vectors of all subwords, but these approaches led to poor performance.\n",
    "\n",
    "Sentence Transformers use specific loss functions to train the average BERT-generated vector of subwords of each sentence in the training set so that two semantically similar sentences are associated with two vectors that are close to each other in terms of (Euclidean) distance and angle. The details of the pre-training and fine-tuning processes of GTE Sentence Transformer are available in Section 3 of [[5]](https://arxiv.org/abs/2308.03281)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. ML Methodology: Logistic Regression, kNN, XGBoost, and FNN**. We use several widely accessible Machine Learning (ML) models to train on the vectors we get from GTE Sentence Transformer on our data so that they can rate a given online comment from 1 to 5 stars to capture consumers' sentiments. Specifically, we use Logistic Regression, k-Nearest Neighbor (kNN), XGBoost, and Feedforward Neural Network (FNN) to build our models.\n",
    "\n",
    "**1.3. Comparing Preceding Works**. There are have been great interests in analyzing consumers' sentiment, many of which are surveyed by Yang, Li, Wang, and Sherratt in Section II of [[8]](https://ieeexplore.ieee.org/document/8970492). In Section III of their paper, the authors also devleop their own sentiment analysis model by manually identifying (sub)words with positive and negative sentiments to create weights attached to BERT-generated vectors from the comments they pre-train. Then they apply Convolutional Neural Network (CNN) get and a modified version of Recurrent Neural Network (RNN) to modify vectors so that they are ideally infludenced by a few words with stong sentiment and remember relationship between words. After that they apply another linear later with hyperbolic tangent activation followed by SoftMax to get a numerical value between 0 and 1. Then the authors of [[8]](https://ieeexplore.ieee.org/document/8970492) use a real-world data set with rating to be able to compare their model with the true rating. Their classification is binary by classifying 1-2 starts to be positive and 3-5 stars to be negative. On the other hand, our approach is greatly simplified thanks to Sentence Transformer, which cuts the pre-training step, which can be significant reduction in cost for deployment. \n",
    "\n",
    "Another work that uses Senternce Transformer can be found in [[3]](https://link.springer.com/chapter/10.1007/978-981-19-5443-6_1) by ranking top tourist spots in Busan, South Korea, by measureing cosine similairties between a vector generated by the given query sentence (e.g., \"It is a beautiful beach\") to the vectors generated from online review comments.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<a id=\"1\">[1]</a> \n",
    "T. Chen, P. Samaranayake, X. Cen, M. Qi, and Y. Lan. (2022). \"**The Impact of Online Reviews on Consumersâ€™ Purchasing Decisions: Evidence From an Eye-Tracking Study**,\" Front Psychol. **13**: 865702.\n",
    "\n",
    "<a id=\"1\">[2]</a> \n",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. (2019). \"**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**,\" Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)\n",
    "\n",
    "<a id=\"1\">[3]</a> \n",
    "M. S. Kim, K. W. Lee, J. W. Lim, D. H. Kim, and S. Hong. (2023). \"**Ranking Roughly Tourist Destinations Using BERT-Based Semantic Search**\", Shakya, S., Du, KL., Ntalianis, K. (eds) Sentiment Analysis and Deep Learning. Advances in Intelligent Systems and Computing, vol 1432. Springer, Singapore.\n",
    "\n",
    "<a id=\"1\">[4]</a>\n",
    "Jiacheng Li, Jingbo Shang, and Julian McAuley. (2022). \"**UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining**,\" Annual Meeting of the Association for Computational Linguistics (ACL)\n",
    "\n",
    "<a id=\"1\">[5]</a>\n",
    "Z. Li, X. Zhang, Y. Zhang, D. Long, P. Xie, and M. Zhang. (2023).\"**Towards General Text Embeddings with Multi-stage Contrastive Learning**,\" arXiv preprint: https://arxiv.org/abs/2308.03281\n",
    "\n",
    "<a id=\"1\">[6]</a>\n",
    "N. Reimers and I. Gurebych. (2019). \"**Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks**,\" Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, p.3982â€“3992, Hong Kong, China, November 3â€“7, 2019.\n",
    "\n",
    "<a id=\"1\">[7]</a>\n",
    "An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian Mcauley. (2023). \"**Personalized Showcases: Generating Multi-Modal Explanations for Recommendations**,\"The 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)\n",
    "\n",
    "<a id=\"1\">[8]</a>\n",
    "L. Yang, Y. Li, J. Wang, and R. S. Sherratt. (2020). \"**Sentiment Analysis for E-Commerce Product Reviews in Chinese Based on Sentiment Lexicon and Deep Learning**,\" IEEE Access **8**: p.23522-23530.\n",
    "\n",
    "<a id=\"1\">[9]</a>\n",
    "Qiang Ye, R. Law, B. Gu, and W. Chen. (2011). \"**The influence of user-generated content on traveler behavior: An empirical investigation on the effects of e-word-of-mouth to hotel online bookings**,\"  Computers in Human Behavior **2**: p.634-639.\n",
    "\n",
    "<a id=\"1\">[10]</a>\n",
    "Zhang and Zhong. (2019). \"**Mining Users Trust From E-Commerce Reviews Based on Sentiment Similarity Analysis**,\" IEEE Access **7**: p.13523-13535.\n",
    "\n",
    "<a id=\"1\">[11]</a>\n",
    "Y. Zhao, L. Wang, H. Tang, and Y. Zhang. (2020). \"**Electronic word-of-mouth and consumer purchase intentions in social e-commerce**,\" Electronic Commerce Research and Applications, **41**: 100980."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
